{
 "cells": [
  {
   "cell_type": "code",
   "id": "39ef37924b321eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T21:03:50.862561Z",
     "start_time": "2025-05-03T21:03:45.571015Z"
    }
   },
   "source": [
    "import gzip, csv, json, re, itertools, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer, InputExample, losses, util\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_DIR      = Path(\"data/GSEA/external_gene_data/store!\")\n",
    "OUTPUT_DIR    = Path(\"output/model\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GENE_FILE     = DATA_DIR / \"rat_genes_consolidated.txt.gz\"\n",
    "PATHWAY_FILE  = DATA_DIR / \"wikipathways_synonyms_Rattus_norvegicus.gmt.gz\"\n",
    "\n",
    "TRAIN_JSONL   = OUTPUT_DIR / \"train.jsonl\"\n",
    "VAL_JSONL     = OUTPUT_DIR / \"val.jsonl\"\n",
    "TEST_JSONL    = OUTPUT_DIR / \"test.jsonl\"\n",
    "\n",
    "BASE_MODEL    = \"michiyasunaga/BioLinkBERT-large\"\n",
    "OUTPUT_FOLDER = OUTPUT_DIR / \"biolinkbert-large-simcse-rat\"\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE     = 256\n",
    "EPOCHS         = 50\n",
    "LEARNING_RATE  = 3e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "DEVICE"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "93f9e636cae01c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T21:03:53.137984Z",
     "start_time": "2025-05-03T21:03:50.979652Z"
    }
   },
   "source": [
    "# # In[1]:\n",
    "# import gzip\n",
    "# import csv\n",
    "# import json\n",
    "# import re\n",
    "# import itertools\n",
    "# import random\n",
    "# from pathlib import Path\n",
    "#\n",
    "# # ---------------- Parameters ----------------\n",
    "# # Fraction of the data to use: 0.1 for 10%, 1.0 for 100%\n",
    "# multiplier = 1\n",
    "#\n",
    "# # Set seed for reproducibility\n",
    "# random.seed(42)\n",
    "#\n",
    "#\n",
    "#\n",
    "# # Regex to extract synonyms inside brackets\n",
    "# bracket_re = re.compile(r\"\\[([^\\]]+)\\]\")\n",
    "#\n",
    "# # ------------- Helper Functions -------------\n",
    "#\n",
    "# def add_pairs(pairs, texts):\n",
    "#     \"\"\"\n",
    "#     Given a list of text entries, strip and add all unique positive pairs.\n",
    "#     \"\"\"\n",
    "#     texts = [t.strip() for t in texts if t and str(t).strip()]\n",
    "#     for a, b in itertools.combinations(set(texts), 2):\n",
    "#         pairs.append((a, b, 1))\n",
    "#\n",
    "#\n",
    "# def sample_splits(splits, multiplier):\n",
    "#     \"\"\"\n",
    "#     If multiplier < 1.0, truncate each list to the given fraction;\n",
    "#     otherwise return unchanged.\n",
    "#     \"\"\"\n",
    "#     if multiplier >= 1.0:\n",
    "#         return splits\n",
    "#     sampled = {}\n",
    "#     for name, items in splits.items():\n",
    "#         k = int(len(items) * multiplier)\n",
    "#         sampled[name] = items[:k]\n",
    "#     return sampled\n",
    "#\n",
    "# # In[2]: Build positive and negative pairs\n",
    "# pos_pairs = []\n",
    "# with gzip.open(GENE_FILE, \"rt\") as fh:\n",
    "#     rdr = csv.DictReader(fh, delimiter='\\t')\n",
    "#     for row in rdr:\n",
    "#         add_pairs(pos_pairs, [\n",
    "#             row.get(\"Gene stable ID\", \"\"),\n",
    "#             row.get(\"Gene name\", \"\"),\n",
    "#             row.get(\"Gene description\", \"\")\n",
    "#         ])\n",
    "#\n",
    "# with gzip.open(PATHWAY_FILE, \"rt\") as fh:\n",
    "#     for line in fh:\n",
    "#         if not line.strip():\n",
    "#             continue\n",
    "#         pathway = re.sub(r\"\\s+\", \" \", line.split(\"\\t\")[0]).strip()\n",
    "#         for grp in bracket_re.findall(line):\n",
    "#             syns = [g.strip() for g in grp.split(\",\") if g.strip()]\n",
    "#             add_pairs(pos_pairs, syns)\n",
    "#             for s in syns:\n",
    "#                 pos_pairs.append((pathway, s, 1))\n",
    "#\n",
    "# # 3) Generate negative pairs\n",
    "# all_texts = list({t for a, b, _ in pos_pairs for t in (a, b)})\n",
    "# pos_set   = {(a, b) for a, b, _ in pos_pairs}\n",
    "# neg_pairs = set()\n",
    "# while len(neg_pairs) < len(pos_pairs):\n",
    "#     a, b = random.sample(all_texts, 2)\n",
    "#     if (a, b) in pos_set or (b, a) in pos_set:\n",
    "#         continue\n",
    "#     if (a, b) in neg_pairs or (b, a) in neg_pairs:\n",
    "#         continue\n",
    "#     neg_pairs.add((a, b))\n",
    "# neg_pairs = [(a, b, 0) for a, b in neg_pairs]\n",
    "#\n",
    "# # 4) Merge, dedupe unordered, shuffle\n",
    "# all_pairs = pos_pairs + neg_pairs\n",
    "# uniq      = {}\n",
    "# for a, b, label in all_pairs:\n",
    "#     key = tuple(sorted((a, b)))\n",
    "#     if key not in uniq:\n",
    "#         uniq[key] = label\n",
    "# unique_pairs = [(a, b, lbl) for (a, b), lbl in uniq.items()]\n",
    "# random.shuffle(unique_pairs)\n",
    "#\n",
    "# # In[3]: Split into train/val/test\n",
    "# N       = len(unique_pairs)\n",
    "# n_test  = int(0.1 * N)\n",
    "# n_rem   = N - n_test\n",
    "# n_val   = n_rem // 9\n",
    "#\n",
    "# splits = {\n",
    "#     \"test\":  unique_pairs[:n_test],\n",
    "#     \"val\":   unique_pairs[n_test:n_test + n_val],\n",
    "#     \"train\": unique_pairs[n_test + n_val:]\n",
    "# }\n",
    "#\n",
    "# # Apply multiplier sampling\n",
    "# splits = sample_splits(splits, multiplier)\n",
    "#\n",
    "# # Write out JSONL and print counts\n",
    "# # No directories needed; files will be created in cwd\n",
    "# for name, path in [(\"train\", TRAIN_JSONL), (\"val\", VAL_JSONL), (\"test\", TEST_JSONL)]:\n",
    "#     with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "#         for a, b, label in splits[name]:\n",
    "#             json.dump({\"text1\": a, \"text2\": b, \"label\": label}, f, ensure_ascii=False)\n",
    "#             f.write(\"\\n\")\n",
    "#\n",
    "# print(f\"✅ Train={len(splits['train']):,}  Val={len(splits['val']):,}  Test={len(splits['test']):,}\")\n",
    "import gzip, csv, json, re, itertools, random\n",
    "from pathlib import Path\n",
    "\n",
    "# … your imports, SEED, DATA_DIR, OUTPUT_DIR, etc. …\n",
    "\n",
    "def add_pairs(pairs, texts):\n",
    "    texts = [t.strip() for t in texts if t and str(t).strip()]\n",
    "    for a, b in itertools.combinations(set(texts), 2):\n",
    "        pairs.append((a, b, 1))\n",
    "\n",
    "# 1) Build pos_pairs and neg_pairs as before\n",
    "pos_pairs = []\n",
    "with gzip.open(GENE_FILE, \"rt\") as fh:\n",
    "    rdr = csv.DictReader(fh)\n",
    "    for row in rdr:\n",
    "        add_pairs(pos_pairs, [\n",
    "            row[\"Gene stable ID\"],\n",
    "            row[\"Gene name\"],\n",
    "            row[\"Gene description\"]\n",
    "        ])\n",
    "\n",
    "bracket_re = re.compile(r\"\\[([^\\]]+)\\]\")\n",
    "with gzip.open(PATHWAY_FILE, \"rt\") as fh:\n",
    "    for line in fh:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        pathway = re.sub(r\"\\s+\", \" \", line.split(\"\\t\")[0]).strip()\n",
    "        for grp in bracket_re.findall(line):\n",
    "            syns = [g.strip() for g in grp.split(\",\") if g.strip()]\n",
    "            add_pairs(pos_pairs, syns)\n",
    "            for s in syns:\n",
    "                pos_pairs.append((pathway, s, 1))\n",
    "\n",
    "all_texts = list({t for a, b, _ in pos_pairs for t in (a, b)})\n",
    "\n",
    "pos_set = {(a, b) for a, b, _ in pos_pairs}\n",
    "neg_pairs = set()\n",
    "while len(neg_pairs) < len(pos_pairs):\n",
    "    a, b = random.sample(all_texts, 2)\n",
    "    if (a, b) in pos_set or (b, a) in pos_set: continue\n",
    "    if (a, b) in neg_pairs or (b, a) in neg_pairs: continue\n",
    "    neg_pairs.add((a, b))\n",
    "neg_pairs = [(a, b, 0) for a, b in neg_pairs]\n",
    "\n",
    "# 2) Merge positives and negatives\n",
    "all_pairs = pos_pairs + neg_pairs\n",
    "\n",
    "# 3) Deduplicate unordered:\n",
    "uniq = {}\n",
    "for a, b, label in all_pairs:\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key not in uniq:\n",
    "        uniq[key] = label\n",
    "# Rebuild a list of unique (a,b,label)\n",
    "unique_pairs = [(a, b, lbl) for (a, b), lbl in uniq.items()]\n",
    "\n",
    "# 4) Shuffle and split\n",
    "random.shuffle(unique_pairs)\n",
    "N      = len(unique_pairs)\n",
    "n_test = int(0.1 * N)\n",
    "n_rem  = N - n_test\n",
    "n_val  = n_rem // 9\n",
    "n_train= n_rem - n_val\n",
    "\n",
    "splits = {\n",
    "    \"test\":  unique_pairs[:n_test],\n",
    "    \"val\":   unique_pairs[n_test:n_test+n_val],\n",
    "    \"train\": unique_pairs[n_test+n_val:]\n",
    "}\n",
    "\n",
    "# 5) Write out\n",
    "for name, path in [(\"train\", TRAIN_JSONL),\n",
    "                   (\"val\",   VAL_JSONL),\n",
    "                   (\"test\",  TEST_JSONL)]:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b, label in splits[name]:\n",
    "            json.dump({\"text1\": a, \"text2\": b, \"label\": label}, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Train={len(splits['train']):,}  Val={len(splits['val']):,}  Test={len(splits['test']):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train=180,795  Val=22,599  Test=22,599\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T21:03:53.805152Z",
     "start_time": "2025-05-03T21:03:53.295943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_pairs(path):\n",
    "    \"\"\"\n",
    "    Load all (text1, text2) pairs from a .jsonl file,\n",
    "    canonicalizing order so (A,B) == (B,A).\n",
    "    Returns a set of tuple pairs.\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            a, b = data[\"text1\"], data[\"text2\"]\n",
    "            # sort the two so order doesn’t matter\n",
    "            pair = tuple(sorted((a, b)))\n",
    "            pairs.add(pair)\n",
    "    return pairs\n",
    "\n",
    "def report_overlap(s1, s2, name1, name2):\n",
    "    overlap = s1 & s2\n",
    "    if overlap:\n",
    "        print(f\"🔴 Overlap between {name1} and {name2}: {len(overlap)} shared pairs\")\n",
    "        for pair in list(overlap)[:10]:\n",
    "            print(\"   \", pair)\n",
    "    else:\n",
    "        print(f\"✅ No overlap between {name1} and {name2}\")\n",
    "\n",
    "def main():\n",
    "    base = Path(\"./output/model/\")\n",
    "    files = {\n",
    "        \"train\": base / \"train.jsonl\",\n",
    "        \"val\":   base / \"val.jsonl\",\n",
    "        \"test\":  base / \"test.jsonl\",\n",
    "    }\n",
    "\n",
    "    # Load\n",
    "    print(\"Loading splits…\")\n",
    "    splits = {name: load_pairs(path) for name, path in files.items()}\n",
    "    for name, s in splits.items():\n",
    "        print(f\"  {name}: {len(s)} unique pairs\")\n",
    "\n",
    "    print(\"\\nChecking overlaps:\")\n",
    "    report_overlap(splits[\"train\"], splits[\"val\"],   \"train\", \"val\")\n",
    "    report_overlap(splits[\"train\"], splits[\"test\"],  \"train\", \"test\")\n",
    "    report_overlap(splits[\"val\"],   splits[\"test\"],  \"val\",   \"test\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "8fc36e80ccf2d146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits…\n",
      "  train: 180795 unique pairs\n",
      "  val: 22599 unique pairs\n",
      "  test: 22599 unique pairs\n",
      "\n",
      "Checking overlaps:\n",
      "✅ No overlap between train and val\n",
      "✅ No overlap between train and test\n",
      "✅ No overlap between val and test\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "16d916e19846ea9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:04:34.732430Z",
     "start_time": "2025-05-03T21:03:53.953534Z"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import (\n",
    "    models,\n",
    "    SentenceTransformer,\n",
    "    InputExample,\n",
    "    losses,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments\n",
    ")\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from transformers import EarlyStoppingCallback\n",
    "import json\n",
    "\n",
    "# ── 0) Re-create your SBERT model & loss ──\n",
    "# (This was originally in Cell 3 of your first snippet)\n",
    "word_model = models.Transformer(BASE_MODEL, max_seq_length=128)\n",
    "word_model.auto_model.gradient_checkpointing_enable()\n",
    "pooling = models.Pooling(\n",
    "    word_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "model = SentenceTransformer(modules=[word_model, pooling], device=DEVICE)\n",
    "loss = losses.ContrastiveLoss(model=model, margin=1.0)\n",
    "\n",
    "# ── 1) Load JSONL splits and build InputExamples ──\n",
    "train_data = [json.loads(l) for l in TRAIN_JSONL.open(\"r\", encoding=\"utf-8\")]\n",
    "val_data   = [json.loads(l) for l in VAL_JSONL.open(\"r\", encoding=\"utf-8\")]\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[d[\"text1\"], d[\"text2\"]], label=d[\"label\"])\n",
    "    for d in train_data\n",
    "]\n",
    "val_examples = [\n",
    "    InputExample(texts=[d[\"text1\"], d[\"text2\"]], label=d[\"label\"])\n",
    "    for d in val_data\n",
    "]\n",
    "\n",
    "# ── 2) Build DataLoaders (for eval_steps/save_steps) ──\n",
    "train_loader = DataLoader(\n",
    "    train_examples,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_examples,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# ── 3) Convert to HF Datasets for Trainer ──\n",
    "train_ds = Dataset.from_list([\n",
    "    {\"text1\": ex.texts[0], \"text2\": ex.texts[1], \"label\": ex.label}\n",
    "    for ex in train_examples\n",
    "])\n",
    "val_ds = Dataset.from_list([\n",
    "    {\"text1\": ex.texts[0], \"text2\": ex.texts[1], \"label\": ex.label}\n",
    "    for ex in val_examples\n",
    "])\n",
    "\n",
    "# ── 4) Evaluator for validation ──\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    val_examples, name='val-eval'\n",
    ")\n",
    "\n",
    "# ── 5) Training arguments with HF-style eval/save & early stopping ──\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=str(OUTPUT_FOLDER),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=len(train_loader),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=len(train_loader),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_val-eval_spearman_cosine\",  # ← match one of the returned keys\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "\n",
    "# ── 6) Build the Trainer with EarlyStoppingCallback ──\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    loss=loss,\n",
    "    evaluator=evaluator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# ── 7) Kick off training ──\n",
    "trainer.train()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0dc394929c94acf9e9a9cfb2f7c5357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31064' max='35350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31064/35350 18:00:35 < 2:29:06, 0.48 it/s, Epoch 43/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val-eval Pearson Cosine</th>\n",
       "      <th>Val-eval Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.115915</td>\n",
       "      <td>0.261697</td>\n",
       "      <td>0.274422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1412</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.081464</td>\n",
       "      <td>0.594961</td>\n",
       "      <td>0.585891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2118</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.076356</td>\n",
       "      <td>0.622305</td>\n",
       "      <td>0.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2824</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.074519</td>\n",
       "      <td>0.633227</td>\n",
       "      <td>0.624896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.074369</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.623555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4236</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.072679</td>\n",
       "      <td>0.644653</td>\n",
       "      <td>0.637608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4942</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.072892</td>\n",
       "      <td>0.643792</td>\n",
       "      <td>0.636271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5648</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.070503</td>\n",
       "      <td>0.658511</td>\n",
       "      <td>0.651232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6354</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.070245</td>\n",
       "      <td>0.661976</td>\n",
       "      <td>0.653238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.069852</td>\n",
       "      <td>0.663715</td>\n",
       "      <td>0.655729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7766</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.663039</td>\n",
       "      <td>0.655915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8472</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.068588</td>\n",
       "      <td>0.672156</td>\n",
       "      <td>0.665477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9178</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.067970</td>\n",
       "      <td>0.675966</td>\n",
       "      <td>0.669738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9884</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.067807</td>\n",
       "      <td>0.678759</td>\n",
       "      <td>0.673056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10590</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.066280</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>0.681274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11296</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.065177</td>\n",
       "      <td>0.694918</td>\n",
       "      <td>0.689669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12002</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.064094</td>\n",
       "      <td>0.701717</td>\n",
       "      <td>0.696475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12708</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.063288</td>\n",
       "      <td>0.706737</td>\n",
       "      <td>0.701622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13414</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.061688</td>\n",
       "      <td>0.715288</td>\n",
       "      <td>0.708846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14120</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.060899</td>\n",
       "      <td>0.720180</td>\n",
       "      <td>0.712606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14826</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.060113</td>\n",
       "      <td>0.723994</td>\n",
       "      <td>0.716035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15532</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.730705</td>\n",
       "      <td>0.720872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16238</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.057850</td>\n",
       "      <td>0.737075</td>\n",
       "      <td>0.726109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16944</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.057618</td>\n",
       "      <td>0.738456</td>\n",
       "      <td>0.726451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.056595</td>\n",
       "      <td>0.743448</td>\n",
       "      <td>0.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18356</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.056366</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.732165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19062</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.055649</td>\n",
       "      <td>0.750909</td>\n",
       "      <td>0.734654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19768</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.056093</td>\n",
       "      <td>0.747941</td>\n",
       "      <td>0.732788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20474</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.751326</td>\n",
       "      <td>0.734086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21180</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.054983</td>\n",
       "      <td>0.755313</td>\n",
       "      <td>0.736782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21886</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.054744</td>\n",
       "      <td>0.757462</td>\n",
       "      <td>0.737630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22592</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.054791</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.737568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23298</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.054668</td>\n",
       "      <td>0.758534</td>\n",
       "      <td>0.738055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24004</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.054226</td>\n",
       "      <td>0.760258</td>\n",
       "      <td>0.739014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24710</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.054312</td>\n",
       "      <td>0.761127</td>\n",
       "      <td>0.739632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25416</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.054119</td>\n",
       "      <td>0.761736</td>\n",
       "      <td>0.740103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26122</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.053830</td>\n",
       "      <td>0.762600</td>\n",
       "      <td>0.739863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26828</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.053852</td>\n",
       "      <td>0.763843</td>\n",
       "      <td>0.740354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27534</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>0.763266</td>\n",
       "      <td>0.739865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28240</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>0.740242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28946</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.053448</td>\n",
       "      <td>0.766089</td>\n",
       "      <td>0.742040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29652</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.053431</td>\n",
       "      <td>0.766408</td>\n",
       "      <td>0.740949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30358</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.053734</td>\n",
       "      <td>0.765324</td>\n",
       "      <td>0.740825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31064</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.053568</td>\n",
       "      <td>0.766083</td>\n",
       "      <td>0.740860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31064, training_loss=0.033217187819293933, metrics={'train_runtime': 64837.5292, 'train_samples_per_second': 139.422, 'train_steps_per_second': 0.545, 'total_flos': 0.0, 'train_loss': 0.033217187819293933, 'epoch': 43.937765205091935})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "test_data = [json.loads(l) for l in TEST_JSONL.open(\"r\", encoding=\"utf-8\")]\n",
    "\n",
    "t1     = [d['text1'] for d in test_data]\n",
    "t2     = [d['text2'] for d in test_data]\n",
    "labels = np.array([d['label'] for d in test_data])\n",
    "\n",
    "word_model_base = models.Transformer(BASE_MODEL, max_seq_length=128)\n",
    "word_model_base.auto_model.gradient_checkpointing_enable()\n",
    "pool_base = models.Pooling(word_model_base.get_word_embedding_dimension(),\n",
    "                           pooling_mode_mean_tokens=True)\n",
    "base_model = SentenceTransformer(modules=[word_model_base, pool_base], device=DEVICE)\n",
    "best_out_dir = OUTPUT_DIR / \"best_model\"            # <-- your target directory\n",
    "\n",
    "# (Re-)create the destination directory if needed\n",
    "if best_out_dir.exists():\n",
    "    shutil.rmtree(best_out_dir)\n",
    "best_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "fine_model = SentenceTransformer(str(best_out_dir), device=DEVICE)\n",
    "\n",
    "batch_size_encode = 128\n",
    "emb_b1 = base_model.encode(t1, batch_size=batch_size_encode,\n",
    "                           convert_to_tensor=True, show_progress_bar=True)\n",
    "emb_b2 = base_model.encode(t2, batch_size=batch_size_encode,\n",
    "                           convert_to_tensor=True, show_progress_bar=False)\n",
    "emb_f1 = fine_model.encode(t1, batch_size=batch_size_encode,\n",
    "                           convert_to_tensor=True, show_progress_bar=False)\n",
    "emb_f2 = fine_model.encode(t2, batch_size=batch_size_encode,\n",
    "                           convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "sims = {\n",
    "    'Base':       util.cos_sim(emb_b1, emb_b2).diag().cpu().numpy(),\n",
    "    'Fine-tuned': util.cos_sim(emb_f1, emb_f2).diag().cpu().numpy()\n",
    "}\n",
    "\n",
    "for name, arr in sims.items():\n",
    "    true_vals  = arr[labels == 1]\n",
    "    false_vals = arr[labels == 0]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.violinplot([true_vals, false_vals], showmeans=True, showmedians=True)\n",
    "    ax.set_title(f\"{name} Similarity Distribution (n={len(test_data)})\")\n",
    "    ax.set_xticks([1, 2]); ax.set_xticklabels([\"True\", \"False\"])\n",
    "    ax.set_ylabel(\"Cosine similarity\")\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    plt.tight_layout()\n",
    "    # Save figure using the number of epochs in filename\n",
    "    save_path = OUTPUT_DIR/ \"PNG\" / f\"{name.lower()}_{EPOCHS}_epochs.png\"\n",
    "    fig.savefig(save_path)\n",
    "    print(f\"Saved plot to {save_path}\")\n",
    "    plt.show()"
   ],
   "id": "60af04a5826ccdb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:05:55.985842Z",
     "start_time": "2025-05-04T15:04:34.919704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# ── 0) Build the BioLinkBERT‐Large base model with mean pooling ──\n",
    "word_model = models.Transformer(\n",
    "    \"michiyasunaga/BioLinkBERT-large\",\n",
    "    max_seq_length=128,\n",
    ")\n",
    "pooling = models.Pooling(\n",
    "    word_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "base_model = SentenceTransformer(\n",
    "    modules=[word_model, pooling],\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# ── 1) Prepare evaluators ──\n",
    "store_dir = OUTPUT_FOLDER # / \"store\"\n",
    "\n",
    "val_data = [json.loads(l) for l in VAL_JSONL.open(\"r\", encoding=\"utf-8\")]\n",
    "val_examples = [\n",
    "    InputExample(texts=[d[\"text1\"], d[\"text2\"]], label=d[\"label\"])\n",
    "    for d in val_data\n",
    "]\n",
    "evaluator_val = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    val_examples, name=\"val-eval\"\n",
    ")\n",
    "\n",
    "test_data = [json.loads(l) for l in TEST_JSONL.open(\"r\", encoding=\"utf-8\")]\n",
    "test_examples = [\n",
    "    InputExample(texts=[d[\"text1\"], d[\"text2\"]], label=d[\"label\"])\n",
    "    for d in test_data\n",
    "]\n",
    "evaluator_test = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    test_examples, name=\"test-eval\"\n",
    ")\n",
    "\n",
    "# ── 2) Locate all checkpoint directories ──\n",
    "ckpts = sorted(\n",
    "    [d for d in store_dir.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint-\")],\n",
    "    key=lambda d: int(d.name.split(\"-\")[1])\n",
    ")\n",
    "\n",
    "# ── 3) Evaluate base model + each checkpoint ──\n",
    "epochs = []       # first entry will be your base model\n",
    "val_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Base model\n",
    "print(\"Evaluating base model…\")\n",
    "metrics_val = evaluator_val(base_model, output_path=None)\n",
    "metrics_test = evaluator_test(base_model, output_path=None)\n",
    "base_val = metrics_val[\"val-eval_spearman_cosine\"]\n",
    "base_test = metrics_test[\"test-eval_spearman_cosine\"]\n",
    "print(f\"  [Base Model] Val Spearman = {base_val:.6f}, Test Spearman = {base_test:.6f}\")\n",
    "epochs.append(BASE_MODEL)\n",
    "val_scores.append(base_val)\n",
    "test_scores.append(base_test)\n",
    "\n",
    "# Fine‐tuned checkpoints (labelled 1, 2, 3, …)\n",
    "for idx, ckpt in enumerate(ckpts, start=1):\n",
    "    print(f\"Evaluating checkpoint at epoch {idx} ({ckpt.name})…\")\n",
    "    model = SentenceTransformer(str(ckpt), device=DEVICE)\n",
    "    metrics_val = evaluator_val(model, output_path=None)\n",
    "    metrics_test = evaluator_test(model, output_path=None)\n",
    "    val_sp = metrics_val[\"val-eval_spearman_cosine\"]\n",
    "    test_sp = metrics_test[\"test-eval_spearman_cosine\"]\n",
    "    print(f\"  [Epoch {idx}] Val Spearman = {val_sp:.6f}, Test Spearman = {test_sp:.6f}\")\n",
    "    epochs.append(idx)\n",
    "    val_scores.append(val_sp)\n",
    "    test_scores.append(test_sp)\n",
    "\n",
    "\n",
    "# ── 4) Pick the best model on the validation set and save it ──\n",
    "import shutil\n",
    "\n",
    "best_val     = max(val_scores)\n",
    "best_index   = val_scores.index(best_val)          # 0 ⇒ base model, ≥1 ⇒ ckpts[best_index-1]\n",
    "best_out_dir = OUTPUT_DIR / \"best_model\"            # <-- your target directory\n",
    "\n",
    "# (Re-)create the destination directory if needed\n",
    "if best_out_dir.exists():\n",
    "    shutil.rmtree(best_out_dir)\n",
    "best_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Select and save\n",
    "if best_index == 0:\n",
    "    print(f\"Best model is the unfine-tuned BASE model (val Spearman = {best_val:.6f})\")\n",
    "    base_model.save(str(best_out_dir))\n",
    "else:\n",
    "    best_ckpt = ckpts[best_index - 1]\n",
    "    print(\n",
    "        f\"Best model is checkpoint {best_index} ({best_ckpt.name}) \"\n",
    "        f\"(val Spearman = {best_val:.6f})\"\n",
    "    )\n",
    "    # SentenceTransformer can save itself straight to a directory\n",
    "    SentenceTransformer(str(best_ckpt)).save(str(best_out_dir))\n",
    "\n",
    "print(f\"✓ Saved best model to {best_out_dir.resolve()}\")"
   ],
   "id": "4e44f2ae1e88371a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model…\n",
      "  [Base Model] Val Spearman = -0.077636, Test Spearman = -0.081831\n",
      "Evaluating checkpoint at epoch 1 (checkpoint-11)…\n",
      "  [Epoch 1] Val Spearman = -0.022254, Test Spearman = -0.030839\n",
      "Evaluating checkpoint at epoch 2 (checkpoint-12)…\n",
      "  [Epoch 2] Val Spearman = -0.021409, Test Spearman = -0.030774\n",
      "Evaluating checkpoint at epoch 3 (checkpoint-22)…\n",
      "  [Epoch 3] Val Spearman = -0.040527, Test Spearman = -0.048524\n",
      "Evaluating checkpoint at epoch 4 (checkpoint-23)…\n",
      "  [Epoch 4] Val Spearman = -0.039751, Test Spearman = -0.047810\n",
      "Evaluating checkpoint at epoch 5 (checkpoint-706)…\n",
      "  [Epoch 5] Val Spearman = 0.274422, Test Spearman = 0.266915\n",
      "Evaluating checkpoint at epoch 6 (checkpoint-1412)…\n",
      "  [Epoch 6] Val Spearman = 0.585891, Test Spearman = 0.574130\n",
      "Evaluating checkpoint at epoch 7 (checkpoint-2118)…\n",
      "  [Epoch 7] Val Spearman = 0.615700, Test Spearman = 0.602231\n",
      "Evaluating checkpoint at epoch 8 (checkpoint-2824)…\n",
      "  [Epoch 8] Val Spearman = 0.624896, Test Spearman = 0.612725\n",
      "Evaluating checkpoint at epoch 9 (checkpoint-3530)…\n",
      "  [Epoch 9] Val Spearman = 0.623555, Test Spearman = 0.609597\n",
      "Evaluating checkpoint at epoch 10 (checkpoint-4236)…\n",
      "  [Epoch 10] Val Spearman = 0.637608, Test Spearman = 0.622730\n",
      "Evaluating checkpoint at epoch 11 (checkpoint-4942)…\n",
      "  [Epoch 11] Val Spearman = 0.636271, Test Spearman = 0.622897\n",
      "Evaluating checkpoint at epoch 12 (checkpoint-5648)…\n",
      "  [Epoch 12] Val Spearman = 0.651232, Test Spearman = 0.633392\n",
      "Evaluating checkpoint at epoch 13 (checkpoint-6354)…\n",
      "  [Epoch 13] Val Spearman = 0.653238, Test Spearman = 0.637955\n",
      "Evaluating checkpoint at epoch 14 (checkpoint-7060)…\n",
      "  [Epoch 14] Val Spearman = 0.655729, Test Spearman = 0.639123\n",
      "Evaluating checkpoint at epoch 15 (checkpoint-7766)…\n",
      "  [Epoch 15] Val Spearman = 0.655915, Test Spearman = 0.644046\n",
      "Evaluating checkpoint at epoch 16 (checkpoint-8472)…\n",
      "  [Epoch 16] Val Spearman = 0.665477, Test Spearman = 0.648763\n",
      "Evaluating checkpoint at epoch 17 (checkpoint-9178)…\n",
      "  [Epoch 17] Val Spearman = 0.669738, Test Spearman = 0.654989\n",
      "Evaluating checkpoint at epoch 18 (checkpoint-9884)…\n",
      "  [Epoch 18] Val Spearman = 0.673056, Test Spearman = 0.660641\n",
      "Evaluating checkpoint at epoch 19 (checkpoint-10590)…\n",
      "  [Epoch 19] Val Spearman = 0.681274, Test Spearman = 0.671234\n",
      "Evaluating checkpoint at epoch 20 (checkpoint-11296)…\n",
      "  [Epoch 20] Val Spearman = 0.689669, Test Spearman = 0.681830\n",
      "Evaluating checkpoint at epoch 21 (checkpoint-12002)…\n",
      "  [Epoch 21] Val Spearman = 0.696475, Test Spearman = 0.687164\n",
      "Evaluating checkpoint at epoch 22 (checkpoint-12708)…\n",
      "  [Epoch 22] Val Spearman = 0.701622, Test Spearman = 0.694293\n",
      "Evaluating checkpoint at epoch 23 (checkpoint-13414)…\n",
      "  [Epoch 23] Val Spearman = 0.708846, Test Spearman = 0.701184\n",
      "Evaluating checkpoint at epoch 24 (checkpoint-14120)…\n",
      "  [Epoch 24] Val Spearman = 0.712606, Test Spearman = 0.703982\n",
      "Evaluating checkpoint at epoch 25 (checkpoint-14826)…\n",
      "  [Epoch 25] Val Spearman = 0.716035, Test Spearman = 0.709768\n",
      "Evaluating checkpoint at epoch 26 (checkpoint-15532)…\n",
      "  [Epoch 26] Val Spearman = 0.720872, Test Spearman = 0.712801\n",
      "Evaluating checkpoint at epoch 27 (checkpoint-16238)…\n",
      "  [Epoch 27] Val Spearman = 0.726109, Test Spearman = 0.718793\n",
      "Evaluating checkpoint at epoch 28 (checkpoint-16944)…\n",
      "  [Epoch 28] Val Spearman = 0.726451, Test Spearman = 0.719178\n",
      "Evaluating checkpoint at epoch 29 (checkpoint-17650)…\n",
      "  [Epoch 29] Val Spearman = 0.729500, Test Spearman = 0.724397\n",
      "Evaluating checkpoint at epoch 30 (checkpoint-18356)…\n",
      "  [Epoch 30] Val Spearman = 0.732165, Test Spearman = 0.726551\n",
      "Evaluating checkpoint at epoch 31 (checkpoint-19062)…\n",
      "  [Epoch 31] Val Spearman = 0.734654, Test Spearman = 0.726668\n",
      "Evaluating checkpoint at epoch 32 (checkpoint-19768)…\n",
      "  [Epoch 32] Val Spearman = 0.732788, Test Spearman = 0.728296\n",
      "Evaluating checkpoint at epoch 33 (checkpoint-20474)…\n",
      "  [Epoch 33] Val Spearman = 0.734086, Test Spearman = 0.730640\n",
      "Evaluating checkpoint at epoch 34 (checkpoint-21180)…\n",
      "  [Epoch 34] Val Spearman = 0.736782, Test Spearman = 0.731716\n",
      "Evaluating checkpoint at epoch 35 (checkpoint-21886)…\n",
      "  [Epoch 35] Val Spearman = 0.737630, Test Spearman = 0.732986\n",
      "Evaluating checkpoint at epoch 36 (checkpoint-22592)…\n",
      "  [Epoch 36] Val Spearman = 0.737568, Test Spearman = 0.733071\n",
      "Evaluating checkpoint at epoch 37 (checkpoint-23298)…\n",
      "  [Epoch 37] Val Spearman = 0.738055, Test Spearman = 0.733524\n",
      "Evaluating checkpoint at epoch 38 (checkpoint-24004)…\n",
      "  [Epoch 38] Val Spearman = 0.739014, Test Spearman = 0.735959\n",
      "Evaluating checkpoint at epoch 39 (checkpoint-24710)…\n",
      "  [Epoch 39] Val Spearman = 0.739632, Test Spearman = 0.735950\n",
      "Evaluating checkpoint at epoch 40 (checkpoint-25416)…\n",
      "  [Epoch 40] Val Spearman = 0.740103, Test Spearman = 0.736086\n",
      "Evaluating checkpoint at epoch 41 (checkpoint-26122)…\n",
      "  [Epoch 41] Val Spearman = 0.739863, Test Spearman = 0.735878\n",
      "Evaluating checkpoint at epoch 42 (checkpoint-26828)…\n",
      "  [Epoch 42] Val Spearman = 0.740354, Test Spearman = 0.737009\n",
      "Evaluating checkpoint at epoch 43 (checkpoint-27534)…\n",
      "  [Epoch 43] Val Spearman = 0.739865, Test Spearman = 0.736283\n",
      "Evaluating checkpoint at epoch 44 (checkpoint-28240)…\n",
      "  [Epoch 44] Val Spearman = 0.740242, Test Spearman = 0.736510\n",
      "Evaluating checkpoint at epoch 45 (checkpoint-28946)…\n",
      "  [Epoch 45] Val Spearman = 0.742040, Test Spearman = 0.739145\n",
      "Evaluating checkpoint at epoch 46 (checkpoint-29652)…\n",
      "  [Epoch 46] Val Spearman = 0.740949, Test Spearman = 0.737462\n",
      "Evaluating checkpoint at epoch 47 (checkpoint-30358)…\n",
      "  [Epoch 47] Val Spearman = 0.740825, Test Spearman = 0.736470\n",
      "Evaluating checkpoint at epoch 48 (checkpoint-31064)…\n",
      "  [Epoch 48] Val Spearman = 0.740860, Test Spearman = 0.737087\n",
      "Best model is checkpoint 45 (checkpoint-28946) (val Spearman = 0.742040)\n",
      "✓ Saved best model to C:\\Users\\Mathieu\\Documents\\Python\\RAG_LUMC\\output\\model\\best_model\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "247a434ff4902fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:05:56.183464Z",
     "start_time": "2025-05-04T16:05:56.179727Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ── Prepare output directory ──\n",
    "output_dir = Path(\"./output/text_files/PNG_HTML\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── Save raw Spearman values for later reuse ──\n",
    "values = {\n",
    "    \"epochs\":     epochs,      # now matches Cell 1\n",
    "    \"val_scores\": val_scores,\n",
    "    \"test_scores\": test_scores\n",
    "}\n",
    "with open(output_dir / \"spearman_values.json\", \"w\") as f:\n",
    "    json.dump(values, f)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:05:56.394707Z",
     "start_time": "2025-05-04T16:05:56.340446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ── Prepare output directory ──\n",
    "output_dir = Path(\"./output/text_files/PNG_HTML\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Numeric positions and labels\n",
    "positions = list(range(len(epochs)))\n",
    "labels = [str(e) for e in epochs]  # e.g. \"michiyasunaga/BioLinkBERT-large\", \"1\", \"2\", …\n",
    "\n",
    "# ── 1) Validation-only plot ──\n",
    "fig_val = go.Figure()\n",
    "fig_val.add_trace(go.Scatter(\n",
    "    x=positions, y=val_scores, mode=\"lines+markers\", name=\"Validation\"\n",
    "))\n",
    "fig_val.update_layout(\n",
    "    title=\"Validation Spearman–Cosine over Base Model + Epochs\",\n",
    "    xaxis=dict(\n",
    "        title=\"Model / Epoch\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=positions,\n",
    "        ticktext=labels,\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(title=\"Spearman Cosine\", range=[min(val_scores) - 0.02, 1.0])\n",
    ")\n",
    "val_path = output_dir / \"spearman_validation.html\"\n",
    "fig_val.write_html(str(val_path), include_plotlyjs=\"cdn\")\n",
    "\n",
    "# ── 2) Test-only plot ──\n",
    "fig_test = go.Figure()\n",
    "fig_test.add_trace(go.Scatter(\n",
    "    x=positions, y=test_scores, mode=\"lines+markers\", name=\"Test\"\n",
    "))\n",
    "fig_test.update_layout(\n",
    "    title=\"Test Spearman–Cosine (in the wild) over Base Model + Epochs\",\n",
    "    xaxis=dict(\n",
    "        title=\"Model / Epoch\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=positions,\n",
    "        ticktext=labels,\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(title=\"Spearman Cosine\", range=[min(test_scores) - 0.02, 1.0])\n",
    ")\n",
    "test_path = output_dir / \"spearman_test.html\"\n",
    "fig_test.write_html(str(test_path), include_plotlyjs=\"cdn\")\n",
    "\n",
    "# ── 3) Combined plot ──\n",
    "fig_comb = go.Figure()\n",
    "fig_comb.add_trace(go.Scatter(\n",
    "    x=positions, y=val_scores, mode=\"lines+markers\", name=\"Validation\"\n",
    "))\n",
    "fig_comb.add_trace(go.Scatter(\n",
    "    x=positions, y=test_scores, mode=\"lines+markers\", name=\"Test\"\n",
    "))\n",
    "fig_comb.update_layout(\n",
    "    title=\"Validation & Test Spearman–Cosine over Base Model + Epochs\",\n",
    "    xaxis=dict(\n",
    "        title=\"Model / Epoch\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=positions,\n",
    "        ticktext=labels,\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(title=\"Spearman Cosine\", range=[min(val_scores + test_scores) - 0.02, 1.0])\n",
    ")\n",
    "comb_path = output_dir / \"spearman_combined.html\"\n",
    "fig_comb.write_html(str(comb_path), include_plotlyjs=\"cdn\")\n",
    "\n",
    "print(f\"Saved:\\n  {val_path}\\n  {test_path}\\n  {comb_path}\")\n"
   ],
   "id": "e65da0ff444a57df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  output\\text_files\\PNG_HTML\\spearman_validation.html\n",
      "  output\\text_files\\PNG_HTML\\spearman_test.html\n",
      "  output\\text_files\\PNG_HTML\\spearman_combined.html\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:58:56.189816Z",
     "start_time": "2025-05-04T16:58:54.591679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"./output/model/best_model\",\n",
    "    repo_id=\"mghuibregtse/biolinkbert-large-simcse-rat\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ],
   "id": "6ee4edd1e46915a7",
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-68179ccf-44db6cf66dde4e89404d7bdf;e28b082a-5efb-46ef-bdae-f2e33a075aff)\n\nRepository Not Found for url: https://huggingface.co/api/models/mghuibregtse/biolinkbert-large-simcse-rat/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m409\u001B[39m     response.raise_for_status()\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\requests\\models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mghuibregtse/biolinkbert-large-simcse-rat/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRepositoryNotFoundError\u001B[39m                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      4\u001B[39m api = HfApi(token=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mHF_TOKEN\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m api.upload_folder(\n\u001B[32m      6\u001B[39m     folder_path=\u001B[33m\"\u001B[39m\u001B[33m./output/model/best_model\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     repo_id=\u001B[33m\"\u001B[39m\u001B[33mmghuibregtse/biolinkbert-large-simcse-rat\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      8\u001B[39m     repo_type=\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      9\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1624\u001B[39m, in \u001B[36mfuture_compatible.<locals>._inner\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_as_future(fn, \u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n\u001B[32m   1623\u001B[39m \u001B[38;5;66;03m# Otherwise, call the function normally\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1624\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4934\u001B[39m, in \u001B[36mHfApi.upload_folder\u001B[39m\u001B[34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001B[39m\n\u001B[32m   4930\u001B[39m commit_operations = delete_operations + add_operations\n\u001B[32m   4932\u001B[39m commit_message = commit_message \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mUpload folder using huggingface_hub\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m4934\u001B[39m commit_info = \u001B[38;5;28mself\u001B[39m.create_commit(\n\u001B[32m   4935\u001B[39m     repo_type=repo_type,\n\u001B[32m   4936\u001B[39m     repo_id=repo_id,\n\u001B[32m   4937\u001B[39m     operations=commit_operations,\n\u001B[32m   4938\u001B[39m     commit_message=commit_message,\n\u001B[32m   4939\u001B[39m     commit_description=commit_description,\n\u001B[32m   4940\u001B[39m     token=token,\n\u001B[32m   4941\u001B[39m     revision=revision,\n\u001B[32m   4942\u001B[39m     create_pr=create_pr,\n\u001B[32m   4943\u001B[39m     parent_commit=parent_commit,\n\u001B[32m   4944\u001B[39m )\n\u001B[32m   4946\u001B[39m \u001B[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001B[39;00m\n\u001B[32m   4947\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m create_pr \u001B[38;5;129;01mand\u001B[39;00m commit_info.pr_url \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1624\u001B[39m, in \u001B[36mfuture_compatible.<locals>._inner\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_as_future(fn, \u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n\u001B[32m   1623\u001B[39m \u001B[38;5;66;03m# Otherwise, call the function normally\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1624\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4193\u001B[39m, in \u001B[36mHfApi.create_commit\u001B[39m\u001B[34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001B[39m\n\u001B[32m   4190\u001B[39m \u001B[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001B[39;00m\n\u001B[32m   4191\u001B[39m _warn_on_overwriting_operations(operations)\n\u001B[32m-> \u001B[39m\u001B[32m4193\u001B[39m \u001B[38;5;28mself\u001B[39m.preupload_lfs_files(\n\u001B[32m   4194\u001B[39m     repo_id=repo_id,\n\u001B[32m   4195\u001B[39m     additions=additions,\n\u001B[32m   4196\u001B[39m     token=token,\n\u001B[32m   4197\u001B[39m     repo_type=repo_type,\n\u001B[32m   4198\u001B[39m     revision=unquoted_revision,  \u001B[38;5;66;03m# first-class methods take unquoted revision\u001B[39;00m\n\u001B[32m   4199\u001B[39m     create_pr=create_pr,\n\u001B[32m   4200\u001B[39m     num_threads=num_threads,\n\u001B[32m   4201\u001B[39m     free_memory=\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001B[39;00m\n\u001B[32m   4202\u001B[39m )\n\u001B[32m   4204\u001B[39m files_to_copy = _fetch_files_to_copy(\n\u001B[32m   4205\u001B[39m     copies=copies,\n\u001B[32m   4206\u001B[39m     repo_type=repo_type,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4210\u001B[39m     endpoint=\u001B[38;5;28mself\u001B[39m.endpoint,\n\u001B[32m   4211\u001B[39m )\n\u001B[32m   4212\u001B[39m \u001B[38;5;66;03m# Remove no-op operations (files that have not changed)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4416\u001B[39m, in \u001B[36mHfApi.preupload_lfs_files\u001B[39m\u001B[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001B[39m\n\u001B[32m   4414\u001B[39m \u001B[38;5;66;03m# Check which new files are LFS\u001B[39;00m\n\u001B[32m   4415\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4416\u001B[39m     _fetch_upload_modes(\n\u001B[32m   4417\u001B[39m         additions=new_additions,\n\u001B[32m   4418\u001B[39m         repo_type=repo_type,\n\u001B[32m   4419\u001B[39m         repo_id=repo_id,\n\u001B[32m   4420\u001B[39m         headers=headers,\n\u001B[32m   4421\u001B[39m         revision=revision,\n\u001B[32m   4422\u001B[39m         endpoint=\u001B[38;5;28mself\u001B[39m.endpoint,\n\u001B[32m   4423\u001B[39m         create_pr=create_pr \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   4424\u001B[39m         gitignore_content=gitignore_content,\n\u001B[32m   4425\u001B[39m     )\n\u001B[32m   4426\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   4427\u001B[39m     e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:680\u001B[39m, in \u001B[36m_fetch_upload_modes\u001B[39m\u001B[34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001B[39m\n\u001B[32m    672\u001B[39m     payload[\u001B[33m\"\u001B[39m\u001B[33mgitIgnore\u001B[39m\u001B[33m\"\u001B[39m] = gitignore_content\n\u001B[32m    674\u001B[39m resp = get_session().post(\n\u001B[32m    675\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/api/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[33ms/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/preupload/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    676\u001B[39m     json=payload,\n\u001B[32m    677\u001B[39m     headers=headers,\n\u001B[32m    678\u001B[39m     params={\u001B[33m\"\u001B[39m\u001B[33mcreate_pr\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m1\u001B[39m\u001B[33m\"\u001B[39m} \u001B[38;5;28;01mif\u001B[39;00m create_pr \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    679\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m680\u001B[39m hf_raise_for_status(resp)\n\u001B[32m    681\u001B[39m preupload_info = _validate_preupload_info(resp.json())\n\u001B[32m    682\u001B[39m upload_modes.update(**{file[\u001B[33m\"\u001B[39m\u001B[33mpath\u001B[39m\u001B[33m\"\u001B[39m]: file[\u001B[33m\"\u001B[39m\u001B[33muploadMode\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m preupload_info[\u001B[33m\"\u001B[39m\u001B[33mfiles\u001B[39m\u001B[33m\"\u001B[39m]})\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    438\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m error_code == \u001B[33m\"\u001B[39m\u001B[33mRepoNotFound\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m    439\u001B[39m     response.status_code == \u001B[32m401\u001B[39m\n\u001B[32m    440\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m error_message != \u001B[33m\"\u001B[39m\u001B[33mInvalid credentials in Authorization header\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    448\u001B[39m     \u001B[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001B[39;00m\n\u001B[32m    449\u001B[39m     \u001B[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001B[39;00m\n\u001B[32m    450\u001B[39m     message = (\n\u001B[32m    451\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Client Error.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    452\u001B[39m         + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    457\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m https://huggingface.co/docs/huggingface_hub/authentication\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    458\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m _format(RepositoryNotFoundError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    461\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m response.status_code == \u001B[32m400\u001B[39m:\n\u001B[32m    462\u001B[39m     message = (\n\u001B[32m    463\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m endpoint:\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mBad request:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    464\u001B[39m     )\n",
      "\u001B[31mRepositoryNotFoundError\u001B[39m: 401 Client Error. (Request ID: Root=1-68179ccf-44db6cf66dde4e89404d7bdf;e28b082a-5efb-46ef-bdae-f2e33a075aff)\n\nRepository Not Found for url: https://huggingface.co/api/models/mghuibregtse/biolinkbert-large-simcse-rat/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case."
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

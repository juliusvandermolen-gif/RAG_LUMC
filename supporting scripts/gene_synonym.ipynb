{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T13:48:07.419314Z",
     "start_time": "2025-03-03T13:48:07.416630Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:54:39.063563Z",
     "start_time": "2025-03-03T13:54:39.060321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root_dir = os.getcwd().split(\"RAG\\\\data\\\\GSEA\")[0] + \"RAG\"\n",
    "target_dir = os.path.join(root_dir, \"data\", \"GSEA\")\n",
    "if os.path.exists(target_dir):\n",
    "    os.chdir(target_dir)\n",
    "else:\n",
    "    print(f\"Error: The directory '{target_dir}' does not exist.\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ],
   "id": "a10fa55cc995c658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Python\\RAG\\Data\\GSEA\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file = \"rat_data.txt.gz\"\n",
    "input_path = os.path.join(\"to_be_converted\", file)\n",
    "output_path = os.path.join(\"to_be_converted\", f\"reordered_{file}\")\n",
    "\n",
    "# Ensure the input file exists\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Error: File '{input_path}' does not exist.\")\n",
    "else:\n",
    "    # Define the columns to extract\n",
    "    cols = [\n",
    "        \"Gene stable ID\",\n",
    "        \"Gene name\",\n",
    "        \"Gene description\",\n",
    "        \"Gene Synonym\",\n",
    "        \"NCBI gene (formerly Entrezgene) description\",\n",
    "        \"NCBI gene (formerly Entrezgene) ID\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Read the gzipped CSV file\n",
    "        df = pd.read_csv(input_path, compression=\"gzip\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading input file:\", e)\n",
    "    else:\n",
    "        # Check if all required columns exist\n",
    "        missing_cols = [col for col in cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(\"Error: The following required columns are missing:\", missing_cols)\n",
    "        else:\n",
    "            # Select and reorder the desired columns\n",
    "            df_selected = df[cols]\n",
    "            \n",
    "            try:\n",
    "                # Write the output to a new gzipped CSV file\n",
    "                df_selected.to_csv(output_path, index=False, compression=\"gzip\")\n",
    "                print(f\"Reordered file saved as {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(\"Error writing output file:\", e)\n"
   ],
   "id": "fd26fb8398b3c284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.chdir('../..')",
   "id": "3c1ffdd6f55f6feb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(os.getcwd())",
   "id": "28325f30360eaeda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Add_Synonyms = True",
   "id": "be19a3b2def135dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "input_file = './data/GSEA/to_be_converted/wikipathways-20241210-gmt-Rattus_norvegicus.gmt'\n",
    "output_file = './data/GSEA/to_be_converted/converted_wikipathways-20241210-gmt-Rattus_norvegicus.gmt'\n",
    "\n",
    "json_file = './data/JSON/ncbi_id_to_symbol.json'\n",
    "\n",
    "# Load the gene ID to symbol dictionary from JSON\n",
    "with open(json_file, 'r') as f:\n",
    "    gene_dict = json.load(f)\n",
    "\n",
    "# Read the GMT input file\n",
    "columns = ['header', 'url'] + [f'gene_{i}' for i in range(1000)]  \n",
    "df = pd.read_csv(input_file, sep='\\t', header=None, names=columns, engine='python', dtype=str, na_filter=False)\n",
    "\n",
    "# Function to replace gene IDs with symbols\n",
    "def replace_gene_ids(gene_id):\n",
    "    return gene_dict.get(gene_id, gene_id)\n",
    "\n",
    "# Apply replacement function to gene columns\n",
    "gene_columns = df.columns[2:]\n",
    "for col in gene_columns:\n",
    "    df[col] = df[col].apply(replace_gene_ids)\n",
    "\n",
    "# Remove empty columns (if all values in a column are empty strings)\n",
    "df = df.loc[:, (df != '').any(axis=0)]\n",
    "\n",
    "# Remove empty rows for gene columns to prevent excess line breaks\n",
    "df = df.apply(lambda x: x.dropna().tolist(), axis=1).apply(pd.Series)\n",
    "\n",
    "# Save to the output file without excess newlines\n",
    "df.to_csv(output_file, sep='\\t', header=False, index=False, lineterminator='\\n')\n",
    "\n",
    "print(f'File conversion completed! Output saved to {output_file}')\n"
   ],
   "id": "47336cb2fad4f985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_file = './data/GSEA/to_be_converted/reordered_rat_data.txt.gz'  \n",
    "output_json = './data/JSON/genes.json' \n",
    "output_file = './data/GSEA/external_gene_data/rat_genes_consolidated.txt.gz' #remove rat"
   ],
   "id": "af13d65d29829a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "ensure_dir(output_json)\n",
    "ensure_dir(output_file)\n",
    "\n",
    "df = pd.read_csv(input_file, compression='gzip')\n",
    "print(f\"Successfully read input file: {input_file}\")\n",
    "\n",
    "df['Gene Synonym'] = df['Gene Synonym'].fillna('').astype(str).str.strip()\n",
    "\n",
    "df['Gene stable ID'] = df['Gene stable ID'].str.strip().str.upper()\n",
    "\n",
    "unique_gene_ids = df['Gene stable ID'].nunique()\n",
    "print(f\"Number of unique 'Gene stable ID's: {unique_gene_ids}\")\n",
    "# Grouping the data and applying transformations\n",
    "grouped = df.groupby('Gene stable ID').agg({\n",
    "    'Gene name': 'first',\n",
    "    'Gene description': 'first',\n",
    "    'Gene Synonym': lambda x: sorted(filter(None, x.unique())),\n",
    "    'NCBI gene (formerly Entrezgene) description': 'first',\n",
    "    'NCBI gene (formerly Entrezgene) ID': 'first'\n",
    "}).reset_index()\n",
    "# Ensure 'Gene Synonyms' are formatted correctly\n",
    "grouped.rename(columns={'Gene Synonym': 'Gene Synonyms'}, inplace=True)\n",
    "grouped['Gene Synonyms'] = grouped['Gene Synonyms'].apply(lambda x: f\"[{','.join(x)}]\" if x else \"[]\")\n",
    "\n",
    "# 1. Removing NCBI description unless Gene description is empty and NCBI description is not\n",
    "grouped['Gene description'] = grouped.apply(\n",
    "    lambda row: row['NCBI gene (formerly Entrezgene) description']\n",
    "    if pd.isna(row['Gene description']) and pd.notna(row['NCBI gene (formerly Entrezgene) description'])\n",
    "    else row['Gene description'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Removing descriptions between []. \n",
    "grouped['Gene description'] = grouped['Gene description'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "\n",
    "#Dropping the NCBI description column as it's no longer needed, and converting the ID to integer.\n",
    "grouped.drop(columns=['NCBI gene (formerly Entrezgene) description'], inplace=True)\n",
    "grouped['NCBI gene (formerly Entrezgene) ID'] = grouped['NCBI gene (formerly Entrezgene) ID'].astype('Int64')\n",
    "\n",
    "consolidated_entries = grouped.shape[0]\n",
    "print(f\"Number of consolidated entries: {consolidated_entries}\")\n",
    "\n",
    "if consolidated_entries != unique_gene_ids:\n",
    "    print(\"Warning: The number of consolidated entries does not match the number of unique 'Gene stable ID's.\")\n",
    "    print(f\"Unique 'Gene stable ID's: {unique_gene_ids}, Consolidated entries: {consolidated_entries}\")\n",
    "else:\n",
    "    print(\"Success: The number of consolidated entries matches the number of unique 'Gene stable ID's.\")\n",
    "\n",
    "genes_list = grouped.to_dict(orient='records')\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f_json:\n",
    "    json.dump(genes_list, f_json, indent=4)\n",
    "print(f\"Consolidated JSON data has been saved to '{output_json}'.\")\n",
    "\n",
    "ordered_columns = [\n",
    "    'Gene stable ID',\n",
    "    'Gene name',\n",
    "    'Gene description',\n",
    "    #'Gene Synonyms',\n",
    "    #'NCBI gene (formerly Entrezgene) ID'\n",
    "]\n",
    "\n",
    "missing_columns = set(ordered_columns) - set(grouped.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing columns in the DataFrame: {missing_columns}\")\n",
    "    exit(1)\n",
    "\n",
    "grouped_ordered = grouped[ordered_columns]\n",
    "\n",
    "grouped_ordered.to_csv(output_file, index=False, sep=',', compression='gzip')\n",
    "print(f\"Consolidated TXT.GZ data has been saved to '{output_file}'.\")\n"
   ],
   "id": "fbc0088bb1a7e543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "genes_json_path = './data/JSON/genes.json'  \n",
    "input_gmt_path = './data/GSEA/to_be_converted/converted_wikipathways-20241210-gmt-Rattus_norvegicus.gmt'\n",
    "output_gmt_path = './data/GSEA/external_gene_data/wikipathways_synonyms_Rattus_norvegicus.gmt.gz'"
   ],
   "id": "7b41694328b7e1ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ensure_dir(output_gmt_path)\n",
    "\n",
    "# Load genes data\n",
    "with open(genes_json_path, 'r', encoding='utf-8') as f_json:\n",
    "    genes_data = json.load(f_json)\n",
    "\n",
    "# Create gene to synonyms mapping\n",
    "gene_to_synonyms = {}\n",
    "for entry in genes_data:\n",
    "    gene_name = entry.get('Gene name')\n",
    "    if gene_name is None:\n",
    "        continue\n",
    "    gene_name = gene_name.strip()\n",
    "    \n",
    "    synonyms_str = entry.get('Gene Synonyms') or ''\n",
    "    synonyms_str = synonyms_str.strip()\n",
    "    if synonyms_str.startswith('[') and synonyms_str.endswith(']'):\n",
    "        synonyms_str = synonyms_str[1:-1]\n",
    "    \n",
    "    synonyms = synonyms_str.split('_') if synonyms_str else []\n",
    "    synonyms = [syn.strip() for syn in synonyms if syn.strip()]\n",
    "    \n",
    "    gene_to_synonyms[gene_name] = synonyms\n",
    "\n",
    "print(f\"Loaded {len(gene_to_synonyms)} genes with synonyms.\")\n",
    "\n",
    "# Process GMT file and save as .gmt.gz\n",
    "def process_gmt(input_path, output_path, gene_synonyms_map):\n",
    "    base_url = \"https://www.wikipathways.org/instance/\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, gzip.open(output_path, 'wt', encoding='utf-8') as outfile:\n",
    "        for line_number, line in enumerate(infile, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  \n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                print(f\"Warning: Line {line_number} in GMT file does not have enough columns. Skipping.\")\n",
    "                continue\n",
    "            pathway_name_full, pathway_url_full, *genes = parts\n",
    "            \n",
    "            if '%' in pathway_name_full:\n",
    "                pathway_name = pathway_name_full.split('%')[0].strip()\n",
    "            else:\n",
    "                pathway_name = pathway_name_full.strip()\n",
    "            \n",
    "            if pathway_url_full.startswith(base_url):\n",
    "                pathway_url = pathway_url_full.replace(base_url, '').strip()\n",
    "            else:\n",
    "                pathway_url = pathway_url_full.strip()\n",
    "            \n",
    "            expanded_genes = []\n",
    "            for gene in genes:\n",
    "                gene = gene.strip()\n",
    "                if not gene:\n",
    "                    continue  \n",
    "                synonyms = gene_synonyms_map.get(gene, [])\n",
    "                if synonyms:\n",
    "                    expanded_gene = f\"[{gene}, \" + \", \".join(synonyms) + \"]\"\n",
    "                else:\n",
    "                    expanded_gene = f\"[{gene}]\"\n",
    "                expanded_genes.append(expanded_gene)\n",
    "            \n",
    "            # **4. Remove duplicate genes while preserving order**\n",
    "            seen = set()\n",
    "            unique_genes = []\n",
    "            for gene in expanded_genes:\n",
    "                if gene not in seen:\n",
    "                    seen.add(gene)\n",
    "                    unique_genes.append(gene)\n",
    "            \n",
    "            # **5. Assemble the new line**\n",
    "            new_line = '\\t'.join([pathway_name, pathway_url] + unique_genes)\n",
    "            outfile.write(new_line + '\\n')\n",
    "            \n",
    "            # **6. Progress Logging**\n",
    "            if line_number % 1000 == 0:\n",
    "                print(f\"Processed {line_number} lines.\")\n",
    "    print(f\"Finished processing GMT file. Output saved to '{output_path}'.\")\n",
    "\n",
    "process_gmt(input_gmt_path, output_gmt_path, gene_to_synonyms)"
   ],
   "id": "a6424487a5fbc2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def initialize_gene_list(excel_file_path=r\".\\data\\GSEA\\genes_of_interest\\PMP22_VS_WT.xlsx\", de_filter_option=\"combined\", test=False):\n",
    "    \"\"\"\n",
    "    Initializes the gene list by processing the Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    - excel_file_path: Path to the Excel file containing gene data.\n",
    "    - de_filter_option: Filter option (\"combined\" or \"separate\").\n",
    "    - test: Boolean flag for test mode.\n",
    "\n",
    "    Returns:\n",
    "    - gene_list_string: Comma-separated string of gene names.\n",
    "    - regulation: Regulation type (\"upregulated\", \"downregulated\", \"combined\").\n",
    "    - num_genes: Number of genes in the list.\n",
    "    \"\"\"\n",
    "    results = process_excel_data(excel_file_path, de_filter_option, test)\n",
    "\n",
    "    if results:\n",
    "        gene_list_string, regulation, num_genes = results[0]\n",
    "    else:\n",
    "        gene_list_string = \"\"\n",
    "        regulation = \"\"\n",
    "        num_genes = 0\n",
    "\n",
    "    return gene_list_string, regulation, num_genes\n",
    "\n",
    "\n",
    "def process_excel_data(excel_file_path, de_filter_option, test):\n",
    "    \"\"\"\n",
    "    Processes the Excel data to filter genes based on DE and FDR thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - excel_file_path: Path to the Excel file.\n",
    "    - de_filter_option: Filter option (\"combined\" or \"separate\").\n",
    "    - test: Boolean flag for test mode.\n",
    "\n",
    "    Returns:\n",
    "    - results: List of tuples containing gene list string, regulation, and number of genes.\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(excel_file_path)\n",
    "    results = []\n",
    "    fdr_threshold = 0.00008802967327\n",
    "\n",
    "    if not test:\n",
    "        max_genes = 250\n",
    "        data = data.iloc[:max_genes]\n",
    "\n",
    "        if de_filter_option == \"combined\":\n",
    "            data = data[data['DE'] != 0]\n",
    "            if not data.empty and data['FDR'].max() <= fdr_threshold:\n",
    "                genes_list = data['X'].tolist()\n",
    "                num_genes = len(genes_list)\n",
    "                unique_de_values = data['DE'].unique()\n",
    "\n",
    "                regulation = (\n",
    "                    \"upregulated\" if len(unique_de_values) == 1 and unique_de_values[0] == 1 else\n",
    "                    \"downregulated\" if len(unique_de_values) == 1 and unique_de_values[0] == -1 else\n",
    "                    \"combined\"\n",
    "                )\n",
    "                results.append((', '.join(genes_list), regulation, num_genes))\n",
    "\n",
    "        elif de_filter_option == \"separate\":\n",
    "            for de_value, regulation_label in [(1, \"upregulated\"), (-1, \"downregulated\")]:\n",
    "                filtered_data = data[data['DE'] == de_value]\n",
    "                if not filtered_data.empty and filtered_data['FDR'].max() <= fdr_threshold:\n",
    "                    genes_list = filtered_data['X'].tolist()\n",
    "                    num_genes = len(genes_list)\n",
    "                    results.append((', '.join(genes_list), regulation_label, num_genes))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid DE filter option. Use 'combined' or 'separate'.\")\n",
    "\n",
    "    else:\n",
    "        # Handling for test=True can be implemented here if needed\n",
    "        pass\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_gene_descriptions(gene_list_string, gene_data_file='rat_genes_consolidated.txt.gz', output_csv='gene_descriptions.csv'):\n",
    "\n",
    "    if not gene_list_string:\n",
    "        print(\"Gene list is empty. No descriptions to extract.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Parse the gene list string into a list\n",
    "    gene_names = [gene.strip() for gene in gene_list_string.split(',') if gene.strip()]\n",
    "    gene_names_set = set(gene_names)  # For faster lookup\n",
    "\n",
    "    print(f\"Total genes to process: {len(gene_names_set)}\")\n",
    "    print(gene_names_set)\n",
    "    # Step 2: Extract gene descriptions from the compressed file\n",
    "    gene_description_dict = {}\n",
    "\n",
    "    if not os.path.exists(gene_data_file):\n",
    "        print(f\"Gene data file '{gene_data_file}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with gzip.open(gene_data_file, 'rt', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                gene_name = row.get('Gene name', '').strip()\n",
    "                if gene_name in gene_names_set:\n",
    "                    description = row.get('Gene description', '').strip()\n",
    "                    gene_description_dict[gene_name] = description\n",
    "                    # Optionally remove found genes to speed up\n",
    "                    gene_names_set.remove(gene_name)\n",
    "                    if not gene_names_set:\n",
    "                        break  # All genes found\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the gene data file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Create a DataFrame for the output\n",
    "    output_data = {\n",
    "        'Gene name': [],\n",
    "        'Gene description': []\n",
    "    }\n",
    "\n",
    "    for gene in gene_names:\n",
    "        description = gene_description_dict.get(gene, \"Description not found\")\n",
    "        output_data['Gene name'].append(gene)\n",
    "        output_data['Gene description'].append(description)\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    # Step 4: Save the output to a new CSV file\n",
    "    try:\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Successfully created '{output_csv}' with {len(output_df)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing the output CSV file: {e}\")\n",
    "\n",
    "    # Optional: Report missing genes\n",
    "    missing_genes = [gene for gene in gene_names if gene not in gene_description_dict]\n",
    "    if missing_genes:\n",
    "        print(f\"The following genes were not found in the gene data file: {', '.join(missing_genes)}\")\n",
    "\n",
    "# Define file paths\n",
    "excel_file_path = r\".\\data\\GSEA\\genes_of_interest\\PMP22_VS_WT.xlsx\"  # Path to your Excel file\n",
    "gene_data_file = r'.\\data\\GSEA\\external_gene_data\\rat_genes_consolidated.txt.gz'  # Path to the compressed gene data file\n",
    "output_csv = r'.\\data\\GSEA\\external_gene_data\\gene_descriptions.csv'  # Desired output CSV file name\n",
    "\n",
    "# Initialize the gene list\n",
    "gene_list_string, regulation, num_genes = initialize_gene_list(\n",
    "    excel_file_path=excel_file_path,\n",
    "    de_filter_option=\"combined\",  # Change to \"separate\" if needed\n",
    "    test=False  # Set to True for test mode\n",
    ")\n",
    "\n",
    "# Extract gene descriptions and save to CSV\n",
    "extract_gene_descriptions(\n",
    "    gene_list_string=gene_list_string,\n",
    "    gene_data_file=gene_data_file,\n",
    "    output_csv=output_csv\n",
    ")\n"
   ],
   "id": "8c76edcfdadc69b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "52021f6a207efaab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:34:37.674331Z",
     "start_time": "2025-04-24T12:13:28.488608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip, csv, json, re, itertools, random, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import models, SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import sentence_transformers.fit_mixin as fit_mixin\n",
    "fit_mixin.DatasetDict = DatasetDict\n",
    "\n",
    "DATA_DIR      = Path(\"data/GSEA/external_gene_data/store!\")\n",
    "GENE_FILE     = DATA_DIR / \"rat_genes_consolidated.txt.gz\"\n",
    "PATHWAY_FILE  = DATA_DIR / \"wikipathways_synonyms_Rattus_norvegicus.gmt.gz\"\n",
    "OUT_JSONL     = Path(\"train_pairs.jsonl\")\n",
    "BASE_MODEL    = \"michiyasunaga/BioLinkBERT-large\"\n",
    "OUTPUT_FOLDER = \"./output/model/biolinkbert-large-simcse-rat\"\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE     = 64            # larger batch improves contrastive negatives\n",
    "EPOCHS         = 25             # train longer with early stopping\n",
    "LEARNING_RATE  = 3e-5          # standard SBERT fine-tune LR\n",
    "WEIGHT_DECAY   = 0.01          # L2 regularization for AdamW\n",
    "WARMUP_RATIO   = 0.1           # 10% of total steps for warmup\n",
    "MAX_GRAD_NORM  = 1.0           # gradient clipping\n",
    "EVAL_RATIO     = 0.1           # 10% of data for validation\n",
    "PATIENCE       = 3             # early-stop after 3 evals without gain\n",
    "SEED           = 42            # fixed seed for reproducibility\n",
    "\n",
    "\n",
    "def add_pairs(pairs, texts):\n",
    "    \"\"\"Add all unique 2-combinations from texts as positive pairs.\"\"\"\n",
    "    texts = [t.strip() for t in texts if t and str(t).strip()]\n",
    "    for a, b in itertools.combinations(set(texts), 2):\n",
    "        pairs.append({\"text1\": a, \"text2\": b})\n",
    "\n",
    "def build_and_write_pairs():\n",
    "    \"\"\"Build positive pairs from genes + pathways, shuffle, and dump to JSONL.\"\"\"\n",
    "    pairs = []\n",
    "    # Gene IDs ↔ name ↔ description\n",
    "    with gzip.open(GENE_FILE, \"rt\") as fh:\n",
    "        rdr = csv.DictReader(fh)\n",
    "        for row in rdr:\n",
    "            add_pairs(pairs, [row[\"Gene stable ID\"],\n",
    "                              row[\"Gene name\"],\n",
    "                              row[\"Gene description\"]])\n",
    "    # Pathway synonyms inside [ … ]\n",
    "    bracket_re = re.compile(r\"\\[([^\\]]+)\\]\")\n",
    "    with gzip.open(PATHWAY_FILE, \"rt\") as fh:\n",
    "        for line in fh:\n",
    "            if not line.strip(): continue\n",
    "            pathway = re.sub(r\"\\s+\", \" \", line.split(\"\\t\")[0]).strip()\n",
    "            for grp in bracket_re.findall(line):\n",
    "                syns = [g.strip() for g in grp.split(\",\") if g.strip()]\n",
    "                add_pairs(pairs, syns)\n",
    "                for s in syns:\n",
    "                    pairs.append({\"text1\": pathway, \"text2\": s})\n",
    "    random.shuffle(pairs)\n",
    "    with OUT_JSONL.open(\"w\") as out:\n",
    "        for ex in pairs:\n",
    "            out.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"✅ Wrote {len(pairs):,} pairs → {OUT_JSONL} \"\n",
    "          f\"({OUT_JSONL.stat().st_size/1e6:.2f} MB)\")\n",
    "\n",
    "def prepare_training():\n",
    "    \"\"\"Load JSONL into InputExample list, build model + loader + MNR loss.\"\"\"\n",
    "    examples = [\n",
    "        InputExample(texts=[d[\"text1\"], d[\"text2\"]])\n",
    "        for d in map(json.loads, OUT_JSONL.open())\n",
    "    ]\n",
    "    loader = DataLoader(examples,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True)\n",
    "\n",
    "    word_model = models.Transformer(BASE_MODEL, max_seq_length=128)\n",
    "    word_model.auto_model.gradient_checkpointing_enable()\n",
    "    pool_model = models.Pooling(\n",
    "        word_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True\n",
    "    )\n",
    "    print(DEVICE)\n",
    "    model = SentenceTransformer(modules=[word_model, pool_model],\n",
    "                                device=DEVICE)\n",
    "\n",
    "    loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    return model, loader, loss\n",
    "\n",
    "def train(model, loader, loss):\n",
    "    total_steps = len(loader) * EPOCHS\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, loss)],\n",
    "        epochs=EPOCHS,\n",
    "        optimizer_params={\"lr\": LEARNING_RATE},\n",
    "        warmup_steps=warmup_steps,\n",
    "        use_amp=True,\n",
    "        output_path=OUTPUT_FOLDER,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_and_write_pairs()\n",
    "    model, loader, loss = prepare_training()\n",
    "    train(model, loader, loss)\n"
   ],
   "id": "3be2c5c80ac503d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 121,533 pairs → train_pairs.jsonl (7.15 MB)\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47450' max='47450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47450/47450 2:20:54, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.990900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training done in 141.1 min\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:14:07.114177Z",
     "start_time": "2025-04-24T17:13:58.641770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "BASE_MODEL = \"michiyasunaga/BioLinkBERT-large\"\n",
    "\n",
    "# a) Transformer module loads the exact HF weights\n",
    "word_model = models.Transformer(\n",
    "    model_name_or_path=BASE_MODEL,\n",
    "    max_seq_length=128\n",
    ")\n",
    "\n",
    "# b) Mean pooling over token embeddings\n",
    "pool_model = models.Pooling(\n",
    "    word_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "# c) Combine into a SentenceTransformer\n",
    "base = SentenceTransformer(\n",
    "    modules=[word_model, pool_model],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "FT_FOLDER = \"./output/model/biolinkbert-large-simcse-rat\"\n",
    "ft = SentenceTransformer(FT_FOLDER, device=device)\n",
    "\n",
    "test_pairs = [\n",
    "    # (text1, text2, expected_cosine)\n",
    "    (\"Alx4\",       \"ALX homeobox 4\",              0.90),\n",
    "    (\"Dgkh\",       \"diacylglycerol kinase, eta\",  0.85),\n",
    "    (\"Prdm11\",     \"PR/SET domain 11\",            0.87),\n",
    "    (\"Spata3\",     \"spermatogenesis associated 3\",0.89),\n",
    "    (\"ENSRNOG00000015159\", \"solute carrier family 9 member A3\", 0.88),\n",
    "    (\"Abcc1\",     \"Irinotecan pathway\",           0.55),\n",
    "    (\"Arrb2\",     \"Wnt signaling pathway\",        0.50),\n",
    "    (\"Ppara\",     \"Nuclear receptors\",            0.52),\n",
    "    (\"gamma-PAK\", \"Regulation of actin cytoskeleton\", 0.48),\n",
    "    (\"Alx4\",   \"glutamate decarboxylase 1\", 0.10),\n",
    "    (\"Dgkh\",   \"Arrb2\",                     0.05),\n",
    "    (\"Prdm11\", \"Irinotecan pathway\",        0.08),\n",
    "]\n",
    "\n",
    "print(f\"{'Text 1':25s} ↔ {'Text 2':30s}  {'Exp':>5s}  {'Base':>6s}  {'FT':>6s}   Result\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "worked = 0\n",
    "total  = len(test_pairs)\n",
    "\n",
    "for text1, text2, exp in test_pairs:\n",
    "    emb_b1 = base.encode(text1, convert_to_tensor=True)\n",
    "    emb_b2 = base.encode(text2, convert_to_tensor=True)\n",
    "    emb_f1 = ft.encode(text1,   convert_to_tensor=True)\n",
    "    emb_f2 = ft.encode(text2,   convert_to_tensor=True)\n",
    "\n",
    "    sim_base = util.cos_sim(emb_b1, emb_b2).item()\n",
    "    sim_ft   = util.cos_sim(emb_f1, emb_f2).item()\n",
    "\n",
    "    dist_base = abs(sim_base - exp)\n",
    "    dist_ft   = abs(sim_ft   - exp)\n",
    "    result = \"WORKED ✅\" if dist_ft < dist_base else \"DIDN’T WORK ❌\"\n",
    "    if dist_ft < dist_base:\n",
    "        worked += 1\n",
    "\n",
    "    print(\n",
    "        f\"{text1:25s} ↔ {text2:30s}  \"\n",
    "        f\"{exp:5.2f}  {sim_base:6.3f}  {sim_ft:6.3f}   {result}\"\n",
    "    )\n",
    "\n",
    "# Overall success rate\n",
    "pct = worked / total * 100\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  {worked}/{total} pairs closer to fine-tuned → {pct:.1f}% worked\")\n"
   ],
   "id": "da7d8a87f4448132",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathi\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1                    ↔ Text 2                            Exp    Base      FT   Result\n",
      "-------------------------------------------------------------------------------------\n",
      "Alx4                      ↔ ALX homeobox 4                   0.90   0.655   0.949   WORKED ✅\n",
      "Dgkh                      ↔ diacylglycerol kinase, eta       0.85   0.804   0.751   DIDN’T WORK ❌\n",
      "Prdm11                    ↔ PR/SET domain 11                 0.87   0.891   0.916   DIDN’T WORK ❌\n",
      "Spata3                    ↔ spermatogenesis associated 3     0.89   0.707   0.892   WORKED ✅\n",
      "ENSRNOG00000015159        ↔ solute carrier family 9 member A3   0.88   0.737   0.580   DIDN’T WORK ❌\n",
      "Abcc1                     ↔ Irinotecan pathway               0.55   0.551   0.591   DIDN’T WORK ❌\n",
      "Arrb2                     ↔ Wnt signaling pathway            0.50   0.849   0.425   WORKED ✅\n",
      "Ppara                     ↔ Nuclear receptors                0.52   0.810   0.569   WORKED ✅\n",
      "gamma-PAK                 ↔ Regulation of actin cytoskeleton   0.48   0.562   0.454   WORKED ✅\n",
      "Alx4                      ↔ glutamate decarboxylase 1        0.10   0.744   0.118   WORKED ✅\n",
      "Dgkh                      ↔ Arrb2                            0.05   0.750   0.090   WORKED ✅\n",
      "Prdm11                    ↔ Irinotecan pathway               0.08   0.742  -0.129   WORKED ✅\n",
      "\n",
      "Summary:\n",
      "  8/12 pairs closer to fine-tuned → 66.7% worked\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"./output/model/biolinkbert-large-simcse-rat\",\n",
    "    repo_id=\"mghuibregtse/biolinkbert-large-simcse-rat\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ],
   "id": "47816f3dc8a0cd01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
